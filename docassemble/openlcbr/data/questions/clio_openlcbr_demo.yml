features:
  javascript: docassemble.openlcbr:data/static/list_collapse.js
  css: docassemble.openlcbr:data/static/list_collapse.css
---
modules:
  - docassemble.openlcbr.lcbr_explain
  - docassemble.openlcbr.ibp_explain
  - docassemble.openlcbr.ibp_data
  - docassemble.base.util
  - docassemble.openlcbr.DATree
  - .DAClio
---
objects:
  - database: DAStaticFile.using(filename="data/sources/trade_secret_cases.yaml")
  - reasoner: DAIBPData
  - explanation: DATree
  - test_case: DAIBPCase
---
mandatory: True
code: |
  # This is the ID for the Trade Secret practice area on the test Clio site
  # It will need to be changed if you are running against a different Clio service.
  trade_secrets = 13144306
  
  # This is the ID for the picklist item in the test Clio account which
  # indicates that the Plaintiff was the winner of the case. It will
  # need to be changed for your implementation.
  plaintiff_winner = 2260081
  
  intro_seen
  load_data
  use_custom_data
  if not use_custom_data:
    for k,v in custom_fields.iteritems():
      if v:
        test_case.factors.append(reasoner.get_factor_id_by_proposition(k))
    test_case.factors.gathered = True
  else:
    # Deleting the test_case.factors attribute forces the interview to collect it again.
    del test_case.factors
---
code: |
  if download_clio_data:
    reasoner.load_model_only(database)
    precedents = get_bulk_custom_fields_in_pa(trade_secrets)
    displayed_clio_data
    for p in precedents:
      # create a case object
      new_case = DAIBPCase()
      new_case.id = p['display_number']      
      for cfv in p['custom_field_values']:
        if cfv['field_name'] == "winner":
          if cfv['value'] == plaintiff_winner:
            new_case.winner = 'Plaintiff'
          else:
            new_case.winner = 'Defendant'
        else:
          if cfv['value'] == True:
            new_case.factors.append(reasoner.get_factor_id_by_proposition(cfv['field_name']))
      new_case.factors.there_is_another = False
      reasoner.add_precedent_case(new_case)
  else:
    reasoner.load(database)
  load_data = True
---
question: Choose a Data Source
subquestion: |
  The analogical reasoning tool will compare your test case to a database of prior
  cases.
  
  Would you like to use the database of cases stored locally to the interview server,
  or would you like to run the analysis against data stored in Clio account?
field: download_clio_data
buttons:
  - "Use Local Data": False
  - "Download Data From Clio": True
---
question: Data Downloaded From Clio
subquestion: |
  The docassemble server has connected with Clio via API and downloaded the following
  matters. Scroll to the bottom and press "continue" to proceed.
  
  % for p in precedents:
  * **${ p['display_number'] }**
  % for cfv in p['custom_field_values']:
  % if cfv['field_name'] == "winner":
  won by
  % if cfv['value'] == plaintiff_winner:
  plaintiff.
  % else:
  defendant.
  % endif
  % endif
  % endfor
  % endfor
field: displayed_clio_data
---
question: docassemble-openlcbr demo
subquestion: |
  This website is a demonstration of the docassemble-openlcbr package being developed
  by Jason Morris of Round Table Law as an ABA Innovation Fellowship project for
  2018/2019.  The fellowship is sponsored by [Clio](http://www.clio.com).
  
  The project is to take openlcbr, which is an open-source tool for
  legal case-based reasoning written by Matthias Grabmair, and to implement it as a
  extension to docassemble.
  
  [Docassemble](https://docassemble.org) is a leading open-source legal expert system and document automation
  platform written by Jonathan Pyle.
  
  The purpose of the project is to explore whether it is possible to use
  reasoning-by-analogy to extend the capabilities of legal expert systems to
  include answering open-textured legal questions.
field: intro_seen
---
question: Choose a Test-Case Data Source
subquestion: |
  Docassemble can be configured to use data located in your law firm's practice
  management software.
  
  Would you like to run the analogical reasoning tool against
  a case that you describe, or would you like to select a matter from a demonstration
  Clio account?
field: use_custom_data
buttons:
  - "Choose an Open Clio Matter": False
  - "Enter Case Details": True
---
if: use_custom_data == True
question: Which of the following factors apply to the test case?
subquestion: |
  Please check off the boxes for all the factors that are true in your test case.
  
  For your convenience, a default set of factors which will demonstrate most of
  the features of the openlcbr analogical reasoner have been pre-selected.
fields:
  - no label: test_case.factors
    datatype: checkboxes
    code: reasoner.factorslist
    default:
      - F21
      - F18
      - F15
      - F14
      - F6
      - F25
      - F16
---
generic object: DATree
code: |
  # This is required when DATree is being used for elements generated by code.
  x.branches.there_is_another = False
---
if: use_custom_data == False
question: Which Clio Matter Do You Want to Predict?
subquestion: |
  This interface is communicating with Clio via API to obtain a list of all open
  cases in the "Trade Secrets" practice area.
fields:
  - Matter: matter
    code: matters
---
code: |
  matters = get_matters_in_pa(trade_secrets)
---
code: |
  custom_fields = get_custom_fields_in_matter(matter)
---
mandatory: True
question: |
  % if explanation.prediction == "p" or explanation.prediction == "d":
  Your issue would be decided for the ${ prediction_word(explanation.prediction) }
  % else:
  The system abstains from making a prediction.
  % endif
subquestion: |
  ${ explanation.display_tree() }
---
code: |
  if download_clio_data:
    explanation = reasoner.predict(test_case)
  else:
    explanation = reasoner.predict(test_case,'trade_secret_test')
---
